{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification:  Acquire data\n",
    "\n",
    "## Goals\n",
    "\n",
    "Data you wish to use in analysis will be stored in a variety of sources. In this lesson, we will review importing data from a csv and via mySQL, and we will also learn how to import data from our local clipboard, a google sheets document, and from an MS Excel file. \n",
    "We will then select one source to use as we continue through the rest of this module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods of Data Acquisition\n",
    "\n",
    "\n",
    "- `read_clipboard`: When you have data copied to your clipboard, you can use pandas to read it into a data frame with `pd.read_clipboard`. This can be useful for quickly transferring data to/from a spreadsheet.   \n",
    "- `read_excel`: This function can be used to create a data frame based on the contents of an Excel spreadsheet.  \n",
    "- `read_csv`: Read from a local csv, or from a the cloud (Google Sheets or AWS S3).    \n",
    "- `read_sql(sql_query, connection_url)`: Read data using a SQL query to a database. You must have the required drivers installed, and a specially formatted url string must be provided.\n",
    "    \n",
    "    ```\n",
    "    # To talk to a mysql database:\n",
    "    python -m pip install pymysql mysql-connector\n",
    "    # the connection url string:\n",
    "    mysql+pymysql://USER:PASSWORD@HOST/DATABASE_NAME\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: A Shared Google Sheet\n",
    "\n",
    "1. Get the shareable link url: https://docs.google.com/spreadsheets/d/BLAHBLAHBLAH/edit#gid=NUMBER\n",
    "\n",
    "2. Turn that into a CSV export URL: Replace `/edit` with `/export`; Add `format=csv` to the beginning of the query string. https://docs.google.com/spreadsheets/d/BLAHBLAHBLAH/export?format=csv&gid=NUMBER:\n",
    "\n",
    "3. Pass it to pd.read_csv, which can take a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/export?format=csv&gid=341089357'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet_url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit#gid=341089357'    \n",
    "\n",
    "csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "csv_export_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_googlesheet = pd.read_csv(csv_export_url)\n",
    "df_googlesheet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RETURN_ID</th>\n",
       "      <th>FILING_TYPE</th>\n",
       "      <th>EIN</th>\n",
       "      <th>TAX_PERIOD</th>\n",
       "      <th>SUB_DATE</th>\n",
       "      <th>TAXPAYER_NAME</th>\n",
       "      <th>RETURN_TYPE</th>\n",
       "      <th>DLN</th>\n",
       "      <th>OBJECT_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9091250</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>591971002</td>\n",
       "      <td>201009</td>\n",
       "      <td>11/30/2011 1:06:39 AM</td>\n",
       "      <td>ANGELUS INC</td>\n",
       "      <td>990</td>\n",
       "      <td>93493316003251</td>\n",
       "      <td>201103169349300325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9091274</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>251713602</td>\n",
       "      <td>201106</td>\n",
       "      <td>11/30/2011 1:09:14 AM</td>\n",
       "      <td>TOUCH-STONE SOLUTIONS INC</td>\n",
       "      <td>990</td>\n",
       "      <td>93493313012311</td>\n",
       "      <td>201113139349301231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9091275</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>232705170</td>\n",
       "      <td>201012</td>\n",
       "      <td>11/30/2011 1:09:16 AM</td>\n",
       "      <td>RONALD MCDONALD HOUSE CHARITIES- PHILADELPHIA ...</td>\n",
       "      <td>990</td>\n",
       "      <td>93493313013011</td>\n",
       "      <td>201113139349301301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9091276</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>581805618</td>\n",
       "      <td>201106</td>\n",
       "      <td>11/30/2011 1:09:19 AM</td>\n",
       "      <td>TORRINGTON VOA ELDERLY HOUSING INC BELL PARK T...</td>\n",
       "      <td>990</td>\n",
       "      <td>93493313013111</td>\n",
       "      <td>201113139349301311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9091277</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>581876019</td>\n",
       "      <td>201106</td>\n",
       "      <td>11/30/2011 1:09:21 AM</td>\n",
       "      <td>HOUSTON VOA INDEPENDENT HOUSING INC HEIGHTS MANOR</td>\n",
       "      <td>990</td>\n",
       "      <td>93493313013161</td>\n",
       "      <td>201113139349301316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RETURN_ID FILING_TYPE        EIN  TAX_PERIOD               SUB_DATE  \\\n",
       "0    9091250       EFILE  591971002      201009  11/30/2011 1:06:39 AM   \n",
       "1    9091274       EFILE  251713602      201106  11/30/2011 1:09:14 AM   \n",
       "2    9091275       EFILE  232705170      201012  11/30/2011 1:09:16 AM   \n",
       "3    9091276       EFILE  581805618      201106  11/30/2011 1:09:19 AM   \n",
       "4    9091277       EFILE  581876019      201106  11/30/2011 1:09:21 AM   \n",
       "\n",
       "                                       TAXPAYER_NAME RETURN_TYPE  \\\n",
       "0                                        ANGELUS INC         990   \n",
       "1                          TOUCH-STONE SOLUTIONS INC         990   \n",
       "2  RONALD MCDONALD HOUSE CHARITIES- PHILADELPHIA ...         990   \n",
       "3  TORRINGTON VOA ELDERLY HOUSING INC BELL PARK T...         990   \n",
       "4  HOUSTON VOA INDEPENDENT HOUSING INC HEIGHTS MANOR         990   \n",
       "\n",
       "              DLN           OBJECT_ID  \n",
       "0  93493316003251  201103169349300325  \n",
       "1  93493313012311  201113139349301231  \n",
       "2  93493313013011  201113139349301301  \n",
       "3  93493313013111  201113139349301311  \n",
       "4  93493313013161  201113139349301316  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the S3 file is private, you will need your S3 configurations setup properly.\n",
    "df_s3 = pd.read_csv('https://s3.amazonaws.com/irs-form-990/index_2011.csv')\n",
    "df_s3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: SQL\n",
    "\n",
    "Create a dataframe from the passengers table in the mySQL database, titanic_db. \n",
    "\n",
    "!!!warning \"Database Credentials\"\n",
    "    It's a bad idea to store your database access credentials (i.e. your username and password) in plaintext in your source code. There are many different ways one could manage secrets like this, but a simple way is to store the values in a python file that is not included along with the rest of your source code. This is what we have done with the `env` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-384d47a3aa0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34mf'mysql+pymysql://{user}:{password}@{host}/{db}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "def get_connection(db, user=env.user, host=env.host, password=env.password):\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('SELECT * FROM passengers', get_connection('titanic_db'))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function that we can reference later to acquire the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_titanic_data():\n",
    "    return pd.read_sql('SELECT * FROM passengers', get_connection('titanic_db'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll store this function in a file named `acquire.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching Your Data\n",
    "Because data acquisition can take time, it's a common practice to write the data locally to a `.csv` file. \n",
    "\n",
    "1. Do whatever you need to do to produce the dataframe that you need. \n",
    "    - For example `df = pd.read_sql('SELECT * FROM passengers', get_connection('titanic_db'))`\n",
    "    - Or your dataframe cound include joins, multiple data sources, etc...\n",
    "    \n",
    "2. Now use `df.to_csv(\"titanic.csv\")` to write that dataframe to the file.\n",
    "3. Now that you've written the csv file, you can use it later in other parts of your pipeline!\n",
    "4. Consider the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_titanic_data():\n",
    "    filename = \"titanic.csv\"\n",
    "    \n",
    "    if os.path.isfile(filename):\n",
    "        return pd.read_csv(filename)\n",
    "    else:\n",
    "        # read the SQL query into a dataframe\n",
    "        df = new_titanic_data()\n",
    "        \n",
    "        # Write that dataframe to disk for later. Called \"caching\" the data for later.\n",
    "        df.to_csv(filename)\n",
    "\n",
    "        # Return the dataframe to the calling code\n",
    "        return df  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "The end product of these exercise is a jupyter notebook (`classification_exercises.ipynb`) and a `acquire.py` file. The notebook will contain all your work as you move through the exercises. The `acquire.py` file should contain the final functions that acquire the data into a pandas dataframe.\n",
    "\n",
    "1. Make a new repo called `classification-exercises` on both GitHub and within your `codeup-data-science` directory. This will be where you do your work for this module. \n",
    "\n",
    "2. Inside of your local `classification-exercises` repo, create a file named `.gitignore` and list the following file names and paths: `env.py`, `.DS_Store`, `.ipynb_checkpoints/`, `__pycache__`, `titanic.csv`, `iris.csv`, and `telco.csv`. Add and commit your `.gitignore` file before moving forward.\n",
    "    \n",
    "3. Now that you are 100% sure that your `.gitignore` file lists `env.py`, create or copy your `env.py` file inside of `classification-exercises`. Running `git status` should show that git is ignoring this file.\n",
    "\n",
    "4. In a jupyter notebook, `classification_exercises.ipynb`, use a python module (pydata or seaborn datasets) containing datasets as a source from the iris data. Create a pandas dataframe, `df_iris`, from this data.\n",
    "\n",
    "    - print the first 3 rows\n",
    "    - print the number of rows and columns (shape)\n",
    "    - print the column names\n",
    "    - print the data type of each column\n",
    "    - print the summary statistics for each of the numeric variables. Would you\n",
    "      recommend rescaling the data based on these statistics?\n",
    "\n",
    "\n",
    "5. Read the `Table1_CustDetails` table from the `Excel_Exercises.xlsx` file into a\n",
    "   dataframe named `df_excel`.\n",
    "\n",
    "    - assign the first 100 rows to a new dataframe, `df_excel_sample`\n",
    "    - print the number of rows of your original dataframe\n",
    "    - print the first 5 column names\n",
    "    - print the column names that have a data type of `object`\n",
    "    - compute the range for each of the numeric variables.\n",
    "\n",
    "\n",
    "6. Read the data from [this google sheet](https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit?usp=sharing) into a dataframe, `df_google`\n",
    "\n",
    "    - print the first 3 rows\n",
    "    - print the number of rows and columns\n",
    "    - print the column names\n",
    "    - print the data type of each column\n",
    "    - print the summary statistics for each of the numeric variables\n",
    "    - print the unique values for each of your categorical variables\n",
    "\n",
    "\n",
    "Make a new python module, `acquire.py` to hold the following data aquisition functions:\n",
    "\n",
    "7. Make a function named `get_titanic_data` that returns the titanic data from the codeup data science database as a pandas data frame. Obtain your data from the _Codeup Data Science Database_. \n",
    "\n",
    "\n",
    "8. Make a function named `get_iris_data` that returns the data from the `iris_db` on the codeup data science database as a pandas data frame. The returned data frame should include the actual name of the species in addition to the `species_id`s. Obtain your data from the _Codeup Data Science Database_. \n",
    "\n",
    "9. Make a function named `get_telco_data` that returns the data from the `telco_churn` database in SQL. In your SQL, be sure to join all 4 tables together, so that the resulting dataframe contains all the contract, payment, and internet service options. Obtain your data from the _Codeup Data Science Database_. \n",
    "\n",
    "10. Once you've got your `get_titanic_data`, `get_iris_data`, and `get_telco_data` functions written, now it's time to add caching to them. To do this, edit the beginning of the function to check for the local filename of `telco.csv`, `titanic.csv`, or `iris.csv`. If they exist, use the .csv file. If the file doesn't exist, then produce the SQL and pandas necessary to create a dataframe, then write the dataframe to a .csv file with the appropriate name. \n",
    "\n",
    "__Be sure to add env.py, titanic.csv, iris.csv, and telco.csv to your .gitignore file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
